"""
Claude SDK æµ‹è¯•è„šæœ¬
ä½¿ç”¨ Anthropic å®˜æ–¹ SDK è°ƒç”¨ ezmodel çš„ Claude æ¨¡å‹
"""

import anthropic
import httpx
import requests
import json
import time
import argparse
from typing import Optional


def create_client(base_url: str, api_key: str) -> anthropic.Anthropic:
    """åˆ›å»ºè‡ªå®šä¹‰ HTTP å®¢æˆ·ç«¯çš„ Anthropic Clientï¼Œé¿å…è¢« WAF æ‹¦æˆª"""
    # è‡ªå®šä¹‰ httpx å®¢æˆ·ç«¯ï¼Œä¿®æ”¹ User-Agent ç­‰è¯·æ±‚å¤´
    custom_http_client = httpx.Client(
        headers={
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        },
        timeout=60.0,
    )
    
    return anthropic.Anthropic(
        api_key=api_key,
        base_url=base_url,
        http_client=custom_http_client,
    )

# é…ç½®é¢œè‰²è¾“å‡º
class Colors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'


def print_header(text: str):
    print(f"\n{Colors.HEADER}{Colors.BOLD}{'='*50}{Colors.ENDC}")
    print(f"{Colors.HEADER}{Colors.BOLD}{text}{Colors.ENDC}")
    print(f"{Colors.HEADER}{Colors.BOLD}{'='*50}{Colors.ENDC}\n")


def print_success(text: str):
    print(f"{Colors.OKGREEN}âœ… {text}{Colors.ENDC}")


def print_fail(text: str):
    print(f"{Colors.FAIL}âŒ {text}{Colors.ENDC}")


def print_info(text: str):
    print(f"{Colors.OKBLUE}â„¹ï¸  {text}{Colors.ENDC}")


def test_basic_message(client: anthropic.Anthropic, model: str) -> bool:
    """æµ‹è¯•åŸºæœ¬çš„æ¶ˆæ¯è¯·æ±‚ï¼ˆéæµå¼ï¼‰"""
    print_header("æµ‹è¯• 1: åŸºæœ¬æ¶ˆæ¯è¯·æ±‚ï¼ˆéæµå¼ï¼‰")
    
    try:
        start_time = time.time()
        
        message = client.messages.create(
            model=model,
            max_tokens=1024,
            messages=[
                {"role": "user", "content": "Hello! Please introduce yourself in one sentence."}
            ]
        )
        
        latency = time.time() - start_time
        
        print_info(f"æ¨¡å‹: {message.model}")
        print_info(f"åœæ­¢åŸå› : {message.stop_reason}")
        print_info(f"è¾“å…¥ tokens: {message.usage.input_tokens}")
        print_info(f"è¾“å‡º tokens: {message.usage.output_tokens}")
        print_info(f"å»¶è¿Ÿ: {latency:.3f}s")
        print(f"\n{Colors.BOLD}å“åº”å†…å®¹:{Colors.ENDC}")
        print(f"{message.content[0].text}\n")
        
        print_success("åŸºæœ¬æ¶ˆæ¯è¯·æ±‚æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except anthropic.APIStatusError as e:
        print_fail(f"åŸºæœ¬æ¶ˆæ¯è¯·æ±‚æµ‹è¯•å¤±è´¥:")
        print_fail(f"  çŠ¶æ€ç : {e.status_code}")
        print_fail(f"  é”™è¯¯ä¿¡æ¯: {e.message}")
        if hasattr(e, 'body'):
            print_fail(f"  å“åº”ä½“: {e.body}")
        if hasattr(e, 'response'):
            print_fail(f"  å“åº”å¤´: {dict(e.response.headers)}")
            try:
                print_fail(f"  åŸå§‹å“åº”: {e.response.text}")
            except:
                pass
        # æ‰“å°æ‰€æœ‰å¯ç”¨å±æ€§
        print_fail(f"  é”™è¯¯å±æ€§: {[attr for attr in dir(e) if not attr.startswith('_')]}")
        return False
    except Exception as e:
        print_fail(f"åŸºæœ¬æ¶ˆæ¯è¯·æ±‚æµ‹è¯•å¤±è´¥: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_streaming_message(client: anthropic.Anthropic, model: str) -> bool:
    """æµ‹è¯•æµå¼æ¶ˆæ¯è¯·æ±‚"""
    print_header("æµ‹è¯• 2: æµå¼æ¶ˆæ¯è¯·æ±‚")
    
    try:
        start_time = time.time()
        first_token_time = None
        full_response = ""
        
        print(f"{Colors.BOLD}æµå¼å“åº”:{Colors.ENDC}")
        
        with client.messages.stream(
            model=model,
            max_tokens=1024,
            messages=[
                {"role": "user", "content": "Write a short 4-line poem about coding."}
            ]
        ) as stream:
            for text in stream.text_stream:
                if first_token_time is None:
                    first_token_time = time.time()
                print(text, end="", flush=True)
                full_response += text
        
        print("\n")
        
        total_latency = time.time() - start_time
        ttfb = first_token_time - start_time if first_token_time else 0
        
        # è·å–æœ€ç»ˆçš„æ¶ˆæ¯å¯¹è±¡ä»¥è·å– usage ä¿¡æ¯
        final_message = stream.get_final_message()
        
        print_info(f"é¦–å­—èŠ‚å»¶è¿Ÿ (TTFB): {ttfb:.3f}s")
        print_info(f"æ€»å»¶è¿Ÿ: {total_latency:.3f}s")
        print_info(f"è¾“å…¥ tokens: {final_message.usage.input_tokens}")
        print_info(f"è¾“å‡º tokens: {final_message.usage.output_tokens}")
        
        print_success("æµå¼æ¶ˆæ¯è¯·æ±‚æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except anthropic.APIStatusError as e:
        print_fail(f"æµå¼æ¶ˆæ¯è¯·æ±‚æµ‹è¯•å¤±è´¥:")
        print_fail(f"  çŠ¶æ€ç : {e.status_code}")
        print_fail(f"  é”™è¯¯ä¿¡æ¯: {e.message}")
        if hasattr(e, 'body'):
            print_fail(f"  å“åº”ä½“: {e.body}")
        if hasattr(e, 'response'):
            print_fail(f"  å“åº”å¤´: {dict(e.response.headers)}")
        return False
    except Exception as e:
        print_fail(f"æµå¼æ¶ˆæ¯è¯·æ±‚æµ‹è¯•å¤±è´¥: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_multi_turn_conversation(client: anthropic.Anthropic, model: str) -> bool:
    """æµ‹è¯•å¤šè½®å¯¹è¯"""
    print_header("æµ‹è¯• 3: å¤šè½®å¯¹è¯")
    
    try:
        messages = []
        
        # ç¬¬ä¸€è½®
        messages.append({"role": "user", "content": "I want to learn Python. Give me 3 tips."})
        
        response1 = client.messages.create(
            model=model,
            max_tokens=1024,
            messages=messages
        )
        
        assistant_response1 = response1.content[0].text
        messages.append({"role": "assistant", "content": assistant_response1})
        
        print(f"{Colors.BOLD}ç”¨æˆ·:{Colors.ENDC} {messages[0]['content']}")
        print(f"{Colors.BOLD}åŠ©æ‰‹:{Colors.ENDC} {assistant_response1[:200]}...\n")
        
        # ç¬¬äºŒè½®
        messages.append({"role": "user", "content": "Can you elaborate on the first tip?"})
        
        response2 = client.messages.create(
            model=model,
            max_tokens=1024,
            messages=messages
        )
        
        assistant_response2 = response2.content[0].text
        
        print(f"{Colors.BOLD}ç”¨æˆ·:{Colors.ENDC} {messages[2]['content']}")
        print(f"{Colors.BOLD}åŠ©æ‰‹:{Colors.ENDC} {assistant_response2[:200]}...\n")
        
        total_input = response1.usage.input_tokens + response2.usage.input_tokens
        total_output = response1.usage.output_tokens + response2.usage.output_tokens
        
        print_info(f"æ€»è¾“å…¥ tokens: {total_input}")
        print_info(f"æ€»è¾“å‡º tokens: {total_output}")
        
        print_success("å¤šè½®å¯¹è¯æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except anthropic.APIStatusError as e:
        print_fail(f"å¤šè½®å¯¹è¯æµ‹è¯•å¤±è´¥: {e.status_code} - {e.message}")
        return False
    except Exception as e:
        print_fail(f"å¤šè½®å¯¹è¯æµ‹è¯•å¤±è´¥: {type(e).__name__}: {e}")
        return False


def test_system_prompt(client: anthropic.Anthropic, model: str) -> bool:
    """æµ‹è¯•ç³»ç»Ÿæç¤ºè¯"""
    print_header("æµ‹è¯• 4: ç³»ç»Ÿæç¤ºè¯")
    
    try:
        message = client.messages.create(
            model=model,
            max_tokens=1024,
            system="You are a friendly pirate. Speak like a pirate.",
            messages=[
                {"role": "user", "content": "How is the weather today?"}
            ]
        )
        
        print(f"{Colors.BOLD}ç³»ç»Ÿæç¤ºè¯:{Colors.ENDC} You are a friendly pirate. Speak like a pirate.")
        print(f"{Colors.BOLD}ç”¨æˆ·:{Colors.ENDC} How is the weather today?")
        print(f"{Colors.BOLD}åŠ©æ‰‹:{Colors.ENDC} {message.content[0].text}\n")
        
        print_info(f"è¾“å…¥ tokens: {message.usage.input_tokens}")
        print_info(f"è¾“å‡º tokens: {message.usage.output_tokens}")
        
        print_success("ç³»ç»Ÿæç¤ºè¯æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except anthropic.APIStatusError as e:
        print_fail(f"ç³»ç»Ÿæç¤ºè¯æµ‹è¯•å¤±è´¥: {e.status_code} - {e.message}")
        return False
    except Exception as e:
        print_fail(f"ç³»ç»Ÿæç¤ºè¯æµ‹è¯•å¤±è´¥: {type(e).__name__}: {e}")
        return False


def test_long_context(client: anthropic.Anthropic, model: str) -> bool:
    """æµ‹è¯•é•¿ä¸Šä¸‹æ–‡å¤„ç†"""
    print_header("æµ‹è¯• 5: é•¿ä¸Šä¸‹æ–‡å¤„ç†")
    
    try:
        # ç”Ÿæˆè¾ƒé•¿çš„è¾“å…¥æ–‡æœ¬
        long_text = """
        The history of artificial intelligence (AI) dates back to the 1950s. 
        In 1956, at the Dartmouth Conference, the term "artificial intelligence" was first coined.
        
        Early AI research focused on symbolic reasoning and expert systems. Notable achievements include:
        1. ELIZA - an early natural language processing program
        2. SHRDLU - a natural language understanding system
        3. Expert systems like MYCIN for medical diagnosis
        
        However, due to computational limitations and algorithmic constraints, AI experienced two "AI winters" 
        in the 1970s and 1980s.
        
        In the 21st century, with the development of big data, cloud computing, and deep learning, 
        AI has experienced a renaissance. The breakthrough performance of AlexNet in the 2012 ImageNet 
        competition marked the beginning of the deep learning era.
        
        Recently, large language models (LLMs) have sparked a new AI revolution. Models like GPT and Claude 
        demonstrate powerful language understanding and generation capabilities, transforming how we interact 
        with technology.
        """ * 3  # é‡å¤3æ¬¡å¢åŠ é•¿åº¦
        
        start_time = time.time()
        
        message = client.messages.create(
            model=model,
            max_tokens=512,
            messages=[
                {"role": "user", "content": f"Please read the following article and summarize it in 3 sentences:\n\n{long_text}"}
            ]
        )
        
        latency = time.time() - start_time
        
        print(f"{Colors.BOLD}è¾“å…¥æ–‡æœ¬é•¿åº¦:{Colors.ENDC} {len(long_text)} å­—ç¬¦")
        print(f"{Colors.BOLD}æ‘˜è¦:{Colors.ENDC}")
        print(f"{message.content[0].text}\n")
        
        print_info(f"è¾“å…¥ tokens: {message.usage.input_tokens}")
        print_info(f"è¾“å‡º tokens: {message.usage.output_tokens}")
        print_info(f"å»¶è¿Ÿ: {latency:.3f}s")
        
        print_success("é•¿ä¸Šä¸‹æ–‡å¤„ç†æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except anthropic.APIStatusError as e:
        print_fail(f"é•¿ä¸Šä¸‹æ–‡å¤„ç†æµ‹è¯•å¤±è´¥: {e.status_code} - {e.message}")
        return False
    except Exception as e:
        print_fail(f"é•¿ä¸Šä¸‹æ–‡å¤„ç†æµ‹è¯•å¤±è´¥: {type(e).__name__}: {e}")
        return False


def test_temperature(client: anthropic.Anthropic, model: str) -> bool:
    """æµ‹è¯•æ¸©åº¦å‚æ•°"""
    print_header("æµ‹è¯• 6: æ¸©åº¦å‚æ•°å¯¹æ¯”")
    
    try:
        prompt = "Describe the color of the sky in one word."
        
        # ä½æ¸©åº¦ï¼ˆæ›´ç¡®å®šæ€§ï¼‰
        response_low = client.messages.create(
            model=model,
            max_tokens=50,
            temperature=0.0,
            messages=[{"role": "user", "content": prompt}]
        )
        
        # é«˜æ¸©åº¦ï¼ˆæ›´éšæœºï¼‰
        response_high = client.messages.create(
            model=model,
            max_tokens=50,
            temperature=1.0,
            messages=[{"role": "user", "content": prompt}]
        )
        
        print(f"{Colors.BOLD}æç¤ºè¯:{Colors.ENDC} {prompt}\n")
        print(f"{Colors.BOLD}æ¸©åº¦ 0.0 å“åº”:{Colors.ENDC} {response_low.content[0].text}")
        print(f"{Colors.BOLD}æ¸©åº¦ 1.0 å“åº”:{Colors.ENDC} {response_high.content[0].text}\n")
        
        print_success("æ¸©åº¦å‚æ•°æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except anthropic.APIStatusError as e:
        print_fail(f"æ¸©åº¦å‚æ•°æµ‹è¯•å¤±è´¥: {e.status_code} - {e.message}")
        return False
    except Exception as e:
        print_fail(f"æ¸©åº¦å‚æ•°æµ‹è¯•å¤±è´¥: {type(e).__name__}: {e}")
        return False


def test_raw_http_request(base_url: str, api_key: str, model: str) -> bool:
    """ä½¿ç”¨åŸå§‹ HTTP è¯·æ±‚æµ‹è¯•ï¼Œç”¨äºè¯Šæ–­é—®é¢˜"""
    print_header("æµ‹è¯• 0: åŸå§‹ HTTP è¯·æ±‚è¯Šæ–­")
    
    url = f"{base_url}/messages"
    
    # æµ‹è¯•ä¸¤ç§è®¤è¯æ–¹å¼
    auth_methods = [
        ("x-api-key (ClaudeåŸç”Ÿ)", {"x-api-key": api_key, "anthropic-version": "2023-06-01"}),
        ("Authorization Bearer (OpenAIå…¼å®¹)", {"Authorization": f"Bearer {api_key}"}),
    ]
    
    payload = {
        "model": model,
        "max_tokens": 100,
        "messages": [
            {"role": "user", "content": "Say hello"}
        ]
    }
    
    success = False
    for auth_name, auth_headers in auth_methods:
        print(f"\n{Colors.BOLD}å°è¯•è®¤è¯æ–¹å¼: {auth_name}{Colors.ENDC}")
        
        headers = {
            "Content-Type": "application/json",
            **auth_headers
        }
        
        try:
            response = requests.post(url, json=payload, headers=headers, timeout=30)
            print_info(f"çŠ¶æ€ç : {response.status_code}")
            print_info(f"å“åº”å¤´: {dict(response.headers)}")
            
            try:
                resp_json = response.json()
                print_info(f"å“åº”ä½“: {json.dumps(resp_json, ensure_ascii=False, indent=2)}")
            except:
                print_info(f"å“åº”ä½“ (æ–‡æœ¬): {response.text[:500]}")
            
            if response.status_code == 200:
                print_success(f"è®¤è¯æ–¹å¼ {auth_name} æˆåŠŸï¼")
                success = True
            else:
                print_fail(f"è®¤è¯æ–¹å¼ {auth_name} å¤±è´¥")
                
        except Exception as e:
            print_fail(f"è¯·æ±‚å¼‚å¸¸: {e}")
    
    # æµ‹è¯•æ¨¡æ‹Ÿ SDK è¯·æ±‚å¤´
    print(f"\n{Colors.BOLD}æµ‹è¯•æ¨¡æ‹Ÿ Anthropic SDK è¯·æ±‚å¤´:{Colors.ENDC}")
    sdk_headers = {
        "Content-Type": "application/json",
        "x-api-key": api_key,
        "anthropic-version": "2023-06-01",
        "User-Agent": f"anthropic-python/0.76.0",  # SDK é»˜è®¤ User-Agent
        "Accept": "application/json",
    }
    print_info(f"è¯·æ±‚å¤´: {sdk_headers}")
    try:
        response = requests.post(url, json=payload, headers=sdk_headers, timeout=30)
        print_info(f"çŠ¶æ€ç : {response.status_code}")
        if response.status_code == 200:
            print_success("æ¨¡æ‹Ÿ SDK è¯·æ±‚å¤´æˆåŠŸï¼")
        else:
            print_fail(f"æ¨¡æ‹Ÿ SDK è¯·æ±‚å¤´å¤±è´¥: {response.text[:500]}")
    except Exception as e:
        print_fail(f"è¯·æ±‚å¼‚å¸¸: {e}")
    
    return success


def run_all_tests(base_url: str, api_key: str, model: str):
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print_header("ezmodel Claude SDK æµ‹è¯•å¥—ä»¶")
    print_info(f"API Base URL: {base_url}")
    print_info(f"Model: {model}")
    print_info(f"API Key: {api_key[:10]}..." if len(api_key) > 10 else f"API Key: {api_key}")
    
    # åˆ›å»ºè‡ªå®šä¹‰ clientï¼ˆä½¿ç”¨ä¿®æ”¹åçš„ User-Agent é¿å… WAF æ‹¦æˆªï¼‰
    client = create_client(base_url, api_key)
    
    results = []
    
    # è¿è¡Œå„é¡¹æµ‹è¯•
    tests = [
        ("åŸå§‹HTTPè¯Šæ–­", lambda: test_raw_http_request(base_url, api_key, model)),
        ("åŸºæœ¬æ¶ˆæ¯è¯·æ±‚", lambda: test_basic_message(client, model)),
        ("æµå¼æ¶ˆæ¯è¯·æ±‚", lambda: test_streaming_message(client, model)),
        ("å¤šè½®å¯¹è¯", lambda: test_multi_turn_conversation(client, model)),
        ("ç³»ç»Ÿæç¤ºè¯", lambda: test_system_prompt(client, model)),
        ("é•¿ä¸Šä¸‹æ–‡å¤„ç†", lambda: test_long_context(client, model)),
        ("æ¸©åº¦å‚æ•°", lambda: test_temperature(client, model)),
    ]
    
    for name, test_func in tests:
        try:
            result = test_func()
            results.append((name, result))
        except Exception as e:
            print_fail(f"{name} æµ‹è¯•å¼‚å¸¸: {e}")
            results.append((name, False))
    
    # æ‰“å°æµ‹è¯•æ€»ç»“
    print_header("æµ‹è¯•ç»“æœæ€»ç»“")
    
    passed = sum(1 for _, r in results if r)
    total = len(results)
    
    for name, result in results:
        status = f"{Colors.OKGREEN}âœ… é€šè¿‡{Colors.ENDC}" if result else f"{Colors.FAIL}âŒ å¤±è´¥{Colors.ENDC}"
        print(f"  {name}: {status}")
    
    print(f"\n{Colors.BOLD}æ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡{Colors.ENDC}")
    
    if passed == total:
        print(f"\n{Colors.OKGREEN}ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ezmodel Claude æœåŠ¡è¿è¡Œæ­£å¸¸ã€‚{Colors.ENDC}")
    else:
        print(f"\n{Colors.WARNING}âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®æˆ–æœåŠ¡çŠ¶æ€ã€‚{Colors.ENDC}")
    
    return passed == total


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ezmodel Claude SDK æµ‹è¯•è„šæœ¬")
    parser.add_argument(
        "--url", 
        type=str, 
        default="https://www.ezmodel.cloud",
        help="API Base URL (é»˜è®¤: https://www.ezmodel.cloud)"
    )
    parser.add_argument(
        "--key", 
        type=str, 
        default="sk-",
        help="API Key"
    )
    parser.add_argument(
        "--model", 
        type=str, 
        default="claude-sonnet-4-20250514",
        help="æ¨¡å‹åç§° (é»˜è®¤: claude-sonnet-4-20250514)"
    )
    parser.add_argument(
        "--test",
        type=str,
        choices=["basic", "stream", "multi", "system", "long", "temp", "all"],
        default="all",
        help="è¿è¡ŒæŒ‡å®šæµ‹è¯• (é»˜è®¤: all)"
    )
    
    args = parser.parse_args()
    
    if args.key == "sk-" or not args.key:
        print(f"{Colors.WARNING}âš ï¸ è­¦å‘Š: æœªæä¾›æœ‰æ•ˆçš„ API Keyï¼Œè¯·ä½¿ç”¨ --key å‚æ•°æŒ‡å®šã€‚{Colors.ENDC}")
    
    # åˆ›å»ºè‡ªå®šä¹‰ clientï¼ˆä½¿ç”¨ä¿®æ”¹åçš„ User-Agent é¿å… WAF æ‹¦æˆªï¼‰
    client = create_client(args.url, args.key)
    
    if args.test == "all":
        run_all_tests(args.url, args.key, args.model)
    else:
        test_map = {
            "basic": lambda: test_basic_message(client, args.model),
            "stream": lambda: test_streaming_message(client, args.model),
            "multi": lambda: test_multi_turn_conversation(client, args.model),
            "system": lambda: test_system_prompt(client, args.model),
            "long": lambda: test_long_context(client, args.model),
            "temp": lambda: test_temperature(client, args.model),
        }
        test_map[args.test]()
